{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.1 - Pyspark Common Operations\n",
    "\n",
    "In this notebook we'll take a look at some common pyspark operation:\n",
    "* join\n",
    "* union\n",
    "* groupBy\n",
    "* toPandas or \"bringing data to local\".\n",
    "\n",
    "Imagine handling an excel file. But the excel file is the size of a small planet. If you conceptualize a spark dataframe as a large excel sheet that has to be handled with special code, you will be able to conceptualize the transformations you wish to perform. If you have an SQL background, you will probably already know some of these operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Join\n",
    "\n",
    "A Join is an operation in [relational databases](https://en.wikipedia.org/wiki/Relational_database) where you combine the registries between two tables. If you have an SQL background you should be familiar with this operation. With a join, you can match where two tables match under any combination of column values. The two tables are usually defined as __left__ and __right__, as these operations are normally non-commutative. The __left__ table is the table you include in the line of code first.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b> The definitions presented here are the very basic definition of the join operations. There are extensions to these models that expand their functionalities.<b>\n",
    "</div>\n",
    "\n",
    "Let's take a look at several common join types in pyspark.\n",
    "\n",
    "### Left join\n",
    "\n",
    "The left join is probably the most common join. It is called this way because the right table is added to the left table when the match is possible. In a sense, the left table is still intact inside the final resulting table. When you add columns to a table in a way all the original content of the original table is still present, we say the left table was __enriched__.\n",
    "\n",
    "Records in an existing column that have no match are filled with a __null__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/left_join.gif\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('adpro').getOrCreate()## Remember: When you create a session in a cluster, it is possible for a sysadmin to follow your resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 6, True, 'pond'),\n",
    "                 ('Goose', 5, True, 'farm'),\n",
    "                 ('Dog', 4, False, 'farm'),\n",
    "                 ('Wolf', 5, False, 'woods')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'speed', 'flies', 'place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTableA.show() ## This is way too slow!! Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('farm', 'pet'),\n",
    "                 ('woods', 'wild')]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['place', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTableB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTableA.join(exampleTableB, on=['place'], how='left') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = exampleTableA.join(exampleTableB, on=['place'], how='left') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show() ## At this point, the interpreter will read exampleTableA, exampleTableB, and join them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can join tables on any number of columns:\n",
    "simpleEntries = [('farm', 'pet', True, 'menace'),\n",
    "                 ('woods', 'wild', False, 'dangerous')]\n",
    "\n",
    "exampleTableC = spark.createDataFrame(simpleEntries, ['place', 'category', 'flies', 'threat_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleTableA.join(exampleTableC, on=['place', 'flies'], how='left').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join\n",
    "\n",
    "The right join is the mirror logic of the left join. We now conserve the information of the entire right table inside the final table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/right_join.gif\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 6, True, 'pond'),\n",
    "                 ('Goose', 5, True, 'farm'),\n",
    "                 ('Dog', 4, False, 'farm'),\n",
    "                 ('Wolf', 5, False, 'woods')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'speed', 'flies', 'place'])\n",
    "\n",
    "simpleEntries = [('farm', 'pet'),\n",
    "                 ('woods', 'wild')]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['place', 'category'])\n",
    "\n",
    "resultDF = exampleTableA.join(exampleTableB, on=['place'], how='right') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show() ## We now have an action. The calculation takes time here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get a table where the only elements conserved from dataframe __A__ are the ones that match elements from the other dataframe. We will miss you, duck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join\n",
    "\n",
    "An inner join conserves the elements from both dataframes that have a perfect match: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/inner_join.gif\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention: we changed the example dataframes so it wold be more explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'floats'),\n",
    "                 ('Goose', 'flies'),\n",
    "                 ('Dog', 'runs'),\n",
    "                 ('Wolf', 'snarls')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'ability'])\n",
    "\n",
    "simpleEntries = [('floats', 'boat'),\n",
    "                 ('snarls', 'wild'),\n",
    "                 ('snores', 'human'),\n",
    "                 ('sleeps', 'lazy')]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['ability', 'object'])\n",
    "\n",
    "resultDF = exampleTableA.join(exampleTableB, on=['ability'], how='inner') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full join\n",
    "\n",
    "As the name implies, a full join will result in a dataframe with all the data from the original dataframes. All rows are conserved, matches will be done when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/full_join.gif\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'floats'),\n",
    "                 ('Goose', 'flies'),\n",
    "                 ('Dog', 'runs'),\n",
    "                 ('Wolf', 'snarls')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'ability'])\n",
    "\n",
    "simpleEntries = [('floats', 'boat'),\n",
    "                 ('snarls', 'wild'),\n",
    "                 ('snores', 'human'),\n",
    "                 ('sleeps', 'lazy')]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['ability', 'object'])\n",
    "\n",
    "resultDF = exampleTableA.join(exampleTableB, on=['ability'], how='full') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins with duplicated lines\n",
    "\n",
    "When you have duplicate lines in one or both dataframes you will also get then in the final dataframe, in every possible combination. For example, if you have 3 matches on the left dataframe and 2 matches on the right dataframe, you will have 3x2=6 records in the final dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/left_join_repetition.gif\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Duck', 'cute'),\n",
    "                 ('Duck', 'cuddly'),\n",
    "                 ('Goose', 'mean'),\n",
    "                 ('Dog', 'cute')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'attribute'])\n",
    "\n",
    "simpleEntries = [('Duck', 'flies'),\n",
    "                 ('Duck', 'swims'),\n",
    "                 ('Goose', 'flies'),]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['name', 'ability'])\n",
    "\n",
    "resultDF = exampleTableA.join(exampleTableB, on=['name'], how='left') ## This is just a transformation. We haven't actually instructed any actions yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b> There are many join logics. We just discussed the most frequent.<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pyspark has the same join logic as SQL](https://i.stack.imgur.com/ObDyr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Union\n",
    "\n",
    "A __Union__, as the name implies, is a \"union\" of two dataframes with the same structure. Roughly speaking it's when you \"pile up\" two dataframes with the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Duck', 'cute'),\n",
    "                 ('Goose', 'mean'),\n",
    "                 ('Dog', 'cute')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'attribute'])\n",
    "\n",
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Wolf', 'snarls'),\n",
    "                 ('Goose', 'flies'),]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['name', 'ability'])\n",
    "\n",
    "resultDF = exampleTableA.union(exampleTableB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b> Notice the final dataframe keeps the schema of the first dataframe! The union is a brute-force operation!<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch the columns of the second dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Duck', 'cute'),\n",
    "                 ('Goose', 'mean'),\n",
    "                 ('Dog', 'cute')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'attribute'])\n",
    "\n",
    "simpleEntries = [('fluffy', 'Duck'),\n",
    "                 ('snarls', 'Wolf'),\n",
    "                 ('flies', 'Goose'),]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['ability', 'name'])\n",
    "\n",
    "resultDF = exampleTableA.union(exampleTableB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to get rid of duplicate entries at some point during your transformations, you can just add the __distinct__ tansformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Duck', 'cute'),\n",
    "                 ('Goose', 'mean'),\n",
    "                 ('Dog', 'cute')]\n",
    "\n",
    "exampleTableA = spark.createDataFrame(simpleEntries, ['name', 'attribute'])\n",
    "\n",
    "simpleEntries = [('Duck', 'fluffy'),\n",
    "                 ('Wolf', 'snarls'),\n",
    "                 ('Goose', 'flies'),]\n",
    "\n",
    "exampleTableB = spark.createDataFrame(simpleEntries, ['name', 'ability'])\n",
    "\n",
    "resultDF = exampleTableA.union(exampleTableB)\n",
    "\n",
    "exampleTableA.union(exampleTableB).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## groupBY\n",
    "\n",
    "A groupBy transformation will collect identical items from a single column or group of columns and will perform an aggregation according to the set of rules you decide. Let's look at an exemple so you can understand it better. We'll use the classic \"Company Personnel sheet\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\", \"Sales\",\"NY\",90000,34,10000),\n",
    "              (\"Michael\", \"Sales\",\"NY\",86000,56,20000),\n",
    "              (\"Robert\", \"Sales\",\"CA\",81000,30,23000),\n",
    "              (\"Maria\", \"Finance\",\"CA\",90000,24,23000),\n",
    "              (\"Raman\", \"Finance\",\"CA\",99000,40,24000),\n",
    "              (\"Scott\", \"Finance\",\"NY\",83000,36,19000),\n",
    "              (\"Jen\", \"Finance\",\"NY\",79000,53,15000),\n",
    "              (\"Jeff\", \"Marketing\",\"CA\",80000,25,18000),\n",
    "              (\"Kumar\", \"Marketing\",\"NY\",91000,50,21000)]\n",
    "\n",
    "df = spark.createDataFrame(simpleData, [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you want to know the total bonus for wach department. You __group__ the data __by__ the __department__ column and sum the __bonus__ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupdf = df.groupBy(\"department\").sum(\"bonus\")\n",
    "\n",
    "groupdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the name of the aggregated column changes name to reflect the type of aggregation you performed.\n",
    "\n",
    "Now let's say you want to know how many employees each department has. Since each line is an entry for a different employee, we can just __count__ the number of lines after aggregation as the number of employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can go deeper: you can break it down by department and affiliate (in this case \"state\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\", \"state\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can keep adding agregations as well. Let's say we want to also find out the maximum Bonus for each department and state, the total bonus, and the average salary. To do so in a more explicit way, we use the __agg__ method.\n",
    "\n",
    "However, first, we need to adress the methods to be used. Whilst __sum__ and __count__ are methods that belong to a pyspark dataframe, you need a way to tell the __agg__ method how to use them. We need to use [spark's sql aggregate functions](https://sparkbyexamples.com/spark/spark-sql-aggregate-functions/).\n",
    "\n",
    "You can import them into your namespace in several ways. Some people just add them to the namespace in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spark.implicits._ ## package that adds sql capabilities to the namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is still a python course, we will respect the Zen of python and import them in the most common way (although I dislike the nomenclature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, again, we want to agreggate the following:\n",
    "1. Number of employees\n",
    "1. Maximum Bonus\n",
    "1. Total Bonus\n",
    "1. Average Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\", \"state\").agg(F.count(\"salary\"),\n",
    "                                      F.max(\"bonus\"),\n",
    "                                      F.sum(\"bonus\"),\n",
    "                                      F.avg(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our comfort we can rename the columns on the go with the __alias__ method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"department\", \"state\").agg(F.count(\"salary\").alias(\"n_employees\"),\n",
    "                                      F.max(\"bonus\").alias(\"max_bonus\"),\n",
    "                                      F.sum(\"bonus\").alias(\"total_bonus\"),\n",
    "                                      F.avg(\"salary\").alias(\"avg_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Defined Functions: UDF\n",
    "\n",
    "Sometimes you may need a function that is not a pyspark custom. You can define a python function and apply it. Not just during an agreggation, but to the entire dataframe as well. In this case, we wish to extrapolate the total expenditure of the company for next year. The company expects to __increase salaries by 5%__ and __increase the bonuses by 10%__. It is a good practice to define the return type of your UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def expenditure_nextyear(salary, bonus, increase_salary=0.05, increase_bonus=0.1):\n",
    "    \"\"\"Extrapolates the expenditure for next year\"\"\"\n",
    "    expenditure = salary * (1.0+increase_salary) + bonus * (1.0+increase_bonus)\n",
    "\n",
    "    return expenditure\n",
    "\n",
    "expendNxtYr = F.udf(expenditure_nextyear, FloatType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recover the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleData = [(\"James\", \"Sales\",\"NY\",90000,34,10000),\n",
    "              (\"Michael\", \"Sales\",\"NY\",86000,56,20000),\n",
    "              (\"Robert\", \"Sales\",\"CA\",81000,30,23000),\n",
    "              (\"Maria\", \"Finance\",\"CA\",90000,24,23000),\n",
    "              (\"Raman\", \"Finance\",\"CA\",99000,40,24000),\n",
    "              (\"Scott\", \"Finance\",\"NY\",83000,36,19000),\n",
    "              (\"Jen\", \"Finance\",\"NY\",79000,53,15000),\n",
    "              (\"Jeff\", \"Marketing\",\"CA\",80000,25,18000),\n",
    "              (\"Kumar\", \"Marketing\",\"NY\",91000,50,21000)]\n",
    "\n",
    "df = spark.createDataFrame(simpleData, [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the __withColumn__ method, we can create a new column with the UDF we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfny = df.withColumn(\"nxt_year\", expendNxtYr(\"salary\", \"bonus\"))\n",
    "\n",
    "dfny.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: pyspark does not have all the amenities of pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfny.agg(F.sum(\"nxt_year\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## toPandas or \"bringing things to local\"\n",
    "\n",
    "When \"working on a cluster\", you are not \"actually on the cluster\". You are on a __Driver Programme__ that interacts with the cluster through a manager. The driver has very limited resources when compared to cluster itself. Your \"local space\" is actually the driver. All the pyspark __transformations__, while executing, manipulate information on the cluster side. As soon as you perform an action, the result of all calculations must go somewhere.\n",
    "\n",
    "The strong suit of pyspark is that it can combine the strengths of scale computing with the versatility of python. Once you manage your transformations on the cluster side, you can place your results in the memory of the local driver. After that, you can start working with pandas (and python), and apply what you already know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Figures/cluster-overview.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas = df.groupBy(\"department\", \"state\").agg(F.count(\"salary\").alias(\"n_employees\"),\n",
    "                                                 F.max(\"bonus\").alias(\"max_bonus\"),\n",
    "                                                 F.sum(\"bonus\").alias(\"total_bonus\"),\n",
    "                                                 F.avg(\"salary\").alias(\"avg_salary\")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x='total_bonus', y='avg_salary', data=dfpandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b> Remember your local driver usually has very little memory compared to your cluster! Only bring limited rows or very aggregated data!<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse is also possible. You can convert a pandas dataframe into a pyspark dataframe and keep doing operations on the cluster side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_cluster = dfpandas[['avg_salary', 'max_bonus']]\n",
    "df_to_cluster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_on_cluster = spark.createDataFrame(df_to_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_on_cluster.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b> This is just scratching the surface of what pyspark can do. You can find more content on moodle.<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over a use-case of what pyspark can actually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
