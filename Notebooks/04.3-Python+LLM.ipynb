{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8c20eb-6eb2-4671-81cd-7f79b2626336",
   "metadata": {},
   "source": [
    "# 04.3- Python + LLMs\n",
    "\n",
    "Using an LLM on a prompt? By now you have most likely have done that.  \n",
    "Copy pasting inputs and outputs and whatnot. Like if it is still 2023.  \n",
    "In this notebook we'll introduce how to get a working python interaction with an LLM that you can later include in your projects.  \n",
    "First, we need some brief definitions:\n",
    "\n",
    "## Intro to Large Language Models (in a programatic sense)\n",
    "\n",
    "### LLMs: Large Language Models\n",
    "A Large Language Model is a type of artificial intelligence model trained on vast amounts of text data to understand and generate human-like language. Think about our Shakespeare example, but on an a much much larger scale. These models are based on deep learning techniques, specifically neural networks, and are capable of performing various natural language processing tasks.\n",
    "\n",
    "### Generative Models\n",
    "While LLMs handle language, other models may generate art, recognize sound (Speech-To-Text), create videos, etc.. For coding purposes, one usually uses an LLM optimized for code, for example. To generate images you might use DALL-E and so on.\n",
    "\n",
    "### Agents\n",
    "Agents are software systems that have a [model at its core](https://developer.nvidia.com/blog/introduction-to-llm-agents/). The agents add memory to the model, making the model \"aware\" of the history of a chat you are having. The model can be any kind of generative model (images, text, etc.). An agent might even use several AI models under the hood, making it multi-task.\n",
    "\n",
    "### Langchain\n",
    "[Langchain](https://en.wikipedia.org/wiki/LangChain) is a software framework designed to make your interaction with AI Models and Agents much easier. It is still in its early stages, so we will use it with caution.\n",
    "\n",
    "### In this course\n",
    "We are going to focus on LLMs suited for coding and data analysis. Please make sure you have the secret key configured for OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9d5fb-87a4-4346-8d4d-af968ff0aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run, or copy paste to terminal and run\n",
    "#!pip install langchain\n",
    "#!pip install -U langchain-openai\n",
    "#You might need to restart the kernel after installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27963a-e8c8-48d4-9d5a-9bb5613c0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use a model from the OpenAI family\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4491d89b-eddf-4a5b-ae78-b27444874268",
   "metadata": {},
   "source": [
    "### Using models with Langchain\n",
    "\n",
    "The `langchain` framework has dedicated modules for specific models. For example, if a model ws developed by OpenAI, there is a dedicated `langchain-openai` set of tools.  \n",
    "This is all still pretty new, as you can see by the version numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa2c95f-0e43-4674-bf06-2a239dec6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bb24a0-835b-4cfb-9603-e728485cb769",
   "metadata": {},
   "source": [
    "In this course we are going to use OpenAI models for three reasons:\n",
    "1. Maturity of the integration between OpeanAI and Langchain\n",
    "2. Quality/Price of the standard model used: gpt-3.5-turbo is cheap and provides adequate results for coding/data analysis.\n",
    "3. Results are more in line with ChatGPT, which is what students may be more familiar with (same family of models)\n",
    "\n",
    "Using the model requires payment, but it is the only paid component of this course. There are open source models that you can deploy on your machine if you are developing a start-up, for example.\n",
    "\n",
    "More resources:\n",
    "* https://github.com/Hannibal046/Awesome-LLM\n",
    "* https://github.com/langchain-ai/langchain/tree/master/cookbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1a89f4-c042-4ae7-8c28-c0041d15c7ff",
   "metadata": {},
   "source": [
    "---\n",
    "### Let's use a model\n",
    "\n",
    "If there is one thing you should take from this course is that Documentation exists for a reason.  \n",
    "Take a look at the OpenAI documentation for models: https://platform.openai.com/docs/models\n",
    "\n",
    "#### Written by chatgpt: (in this course we will focus only on the first four)\n",
    "When using a large language model like GPT-3, there are several key parameters that you can adjust to customize the behavior of the model for your specific task or application. Here are some of the main parameters:\n",
    "* Model Size: The size of the model refers to the number of parameters it has. Larger models typically have more capacity to understand and generate complex language, but they also require more computational resources.\n",
    "* Context Length: This parameter determines the amount of context the model considers when generating responses. A longer context allows the model to better understand the input, but it also increases computational requirements.\n",
    "* Temperature: Temperature is a parameter that controls the randomness of the model's output. Higher temperature values (e.g., 0.8) lead to more creative and diverse responses, while lower values (e.g., 0.2) make the output more deterministic and focused.\n",
    "* Max Tokens: This parameter limits the length of the generated output. It can be useful to prevent overly long responses.\n",
    "* Top-k and Top-p/Nucleus Sampling: These parameters control the diversity of the generated output. Top-k sampling selects the top k most likely next tokens, while top-p (nucleus) sampling selects tokens until the cumulative probability exceeds a certain threshold p.\n",
    "* Frequency Penalties: You can apply penalties or bonuses based on the frequency of tokens in the output. This can be used to encourage or discourage the use of specific words or phrases.\n",
    "* Prompt Engineering: Crafting a well-defined and clear prompt can significantly impact the quality and relevance of the model's responses. Experimenting with different prompts can help achieve the desired output.\n",
    "* Fine-tuning: If you have specific requirements for your task, you can fine-tune the model on a custom dataset. Fine-tuning allows the model to learn task-specific patterns and improve performance.\n",
    "* Resource Allocation: Depending on the hardware you are using, you may need to adjust parameters to fit within the available resources. Larger models and longer contexts require more memory and processing power.\n",
    "* API Usage: When using a language model through an API, there might be additional parameters or constraints specific to the API. Understanding the API documentation is crucial for effective utilization.\n",
    "\n",
    "It's essential to experiment with these parameters to find the right balance for your specific use case. Keep in mind that tweaking parameters may require some trial and error to achieve the desired results.\n",
    "\n",
    "---\n",
    "\n",
    "Since we are going to do introductory level requests, and are looking mostly for coding solutions, we will keep the temperature low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b08063c-da5b-4f5e-96ff-e7833ebfc97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1) #Check shift+Tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079c7f5-1dbf-4f97-a98e-ecad814dae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Play around with the autocomplete. Check what methods and attributes our instance \"llm\" has.\n",
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7eade2-979a-4d91-b327-55a241994c30",
   "metadata": {},
   "source": [
    "#### The most basic interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2837e-7d83-4767-8c91-9d87ba314338",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke('Here is a fun fact about Optimus Prime:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfe150-74b7-4f26-ad06-1450f407d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke('Which music album is considered the best of all time by critics?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fda83-4178-4b26-8945-0bdd8f378734",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f73bf-5311-47f4-b932-4ae7b611a205",
   "metadata": {},
   "source": [
    "#### For better printing on a Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716e3c1a-06a4-43cb-9d4e-00372d10b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca0737-b920-45b1-b985-bda812726320",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71b2e4-b47f-48fd-bffc-2eaba662d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke('Please remove albums released later than 1990.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59da00b-0dd1-46db-a101-c8267a766633",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419838a-66e2-464d-8e7d-bb4a312d2022",
   "metadata": {},
   "source": [
    "The LLM has no recollection of your previous message. We are doing \"Zero-Shot calls\" to the model, meaning there is no notion of state. In other words, the instance you started has no memory of past prompts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
