{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07.2 - Pyspark use-case\n",
    "_aka_  \n",
    "__We get rid of our training wheels__\n",
    "\n",
    "Let's do a micro-project. Before you can use the data to train a model, you will need to cross databases, join registries, add columns, replace data...\n",
    "\n",
    "So far in this course we have been using complete and clean datasets.\n",
    "\n",
    "\n",
    "We have a collection of databases (just 2, for the sake of time) on the \"sparkfiles\" directory. The goal is to create a final dataset on top of which we can run some ML model.\n",
    "\n",
    "The goal is to predict the housing __SalePrice__ target based on other KPIs we have available.\n",
    "\n",
    "The __houses_A-L.csv__ database is the listing of houses from neighbourhoods whose name starts in the range A to L. It was collected by one team at your company. The __houses_M-Z.csv__ was collected by another team. let's take a look and see if we can use a union on both databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('adpro').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfal = spark.read.csv(\"../sparkfiles/houses_A-L.csv\")\n",
    "dfal.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike pandas, spark needs a little more guidance when reading from csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfal = spark.read.option(\"header\", True).csv(\"../sparkfiles/houses_A-L.csv\")\n",
    "dfal.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfal.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, in a company enviroment, you don't keep data in csvs. The company usually resorts to a \"data lake\" solution: a large database open to the members of the company with tiers of security. Imagine that you work in customer relations. The GDPR allows you to have access to customer information, provided you sign a [Non-Disclosure Agreement (NDA)](https://en.wikipedia.org/wiki/Non-disclosure_agreement). Breeching such an agreemnt has harsh consequences.\n",
    "\n",
    "Technical teams often do not have access to customer information, but to activity generated by customers in the assets of the company. For example, at a telco company, technical teams have access to how many calls are being made through a cell tower (a company asset), but no idea who is doing them.\n",
    "\n",
    "So, it is possible for both the customer and the technical table to reside in the same data lake, but with diferent permission settings. Thus, the data lake is __GDPR compliant__.\n",
    "\n",
    "Pyspark offers the possibility to register a temporary table on the cluster side. We will first register our two tables, to simulate we are accessing a company's data lake. Only, instead of several TB of information, we are doing kB tables for the sake of speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfal.registerTempTable(\"houses_a_l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmz = spark.read.option(\"header\", True).csv(\"../sparkfiles/houses_M-Z.csv\")\n",
    "dfmz.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someone placed a \";\" instead of a \",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmz = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(\"../sparkfiles/houses_M-Z.csv\")\n",
    "dfmz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmz.registerTempTable(\"houses_m_z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this moment, your entry point to the datalake would could be done via SQL. If you are unfamiliar with SQL, don't worry. The advantage of pyspark is hat you can select the entire table and do just a small preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"SELECT * FROM houses_a_l LIMIT 5\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.sql(\"SELECT * FROM houses_m_z LIMIT 5\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Question: how can we combine the two tables in a spark dataframe and can we do it right away?<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though our prediction target is the __SalePrice__, it is not important for it to be at the end. Pyspark rarely cares about column order. Since our columns have a 1-1 match between both dataframes, we can easily do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>We now want the entire table, so we need to rerun the SQL query<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"SELECT * FROM houses_a_l\")\n",
    "df2 = spark.sql(\"SELECT * FROM houses_m_z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.select(sorted(df1.columns))\n",
    "df2 = df2.select(sorted(df2.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>Remember: tables will usually have hundreds of columns. It is too hard to change them by hand.<b>\n",
    "</div>\n",
    "        \n",
    "It is now safe to union the dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1.union(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make SQL queries on top of __df__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"houses\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM houses WHERE kitchenqual='Gd'\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we have python, we can do aggregations. Remember, aggregations are done cluster-side. Returning tens of lines is easier than returning millions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas = df.groupBy(\"TotalRooms\").count().toPandas()\n",
    "dfpandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    <b>'count' is a reserved pandas name. Never call a column the name of a pandas operation!<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas.columns = [\"TotalRooms\", \"Counts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TotalRooms is a string instead of an integer! We need to take notice of the types of the columns going forwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas['TotalRooms'] = pd.to_numeric(dfpandas['TotalRooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpandas.sort_values(by='TotalRooms', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='TotalRooms', y='Counts', data=dfpandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "American houses sure are big!\n",
    "\n",
    "Our final dataframe is now almost ready to be modelled. We need to make sure the variables have their correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whlist some variables are definetely categorical, others are numerical. We need to enforce the columns with numerical values to be contain numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df.withColumn(\"LivingArea\", F.col(\"LivingArea\").cast(\"bigint\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this is not easy to automate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_change = ['LivingArea', 'LotArea', 'MoSold', 'NumberBedroom', 'OverallCond', 'SalePrice', 'TotalRooms', 'YearBuilt', 'YearRemodAdd', 'YrSold']\n",
    "\n",
    "for col in cols_to_change:\n",
    "    df = df.withColumn(col, F.col(col).cast(\"bigint\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"housing_market\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a table at our cluster ready to perform training.\n",
    "\n",
    "Pyspark has a vectorized ML suite. The same functions you know from scikit-learn are adapted to a distributed environment.\n",
    "\n",
    "The methodology is the same. What differs is what is under the hood. \n",
    "\n",
    "---\n",
    "## Spark MLlib: classification exercise\n",
    "\n",
    "I would like to thank professor Susana Brandão for the help in this part.\n",
    "\n",
    "Spark has a [Machine Learning library](https://spark.apache.org/mllib/) that enables you to do machine learning at scale.\n",
    "\n",
    "__For a simple example ready to go, with numerical variables alone, [check this notebook](../Examples/04-SimpleLinearRegressionPyspark.ipynb).__\n",
    "\n",
    "Let's \"retrieve\" the dataframe from our data lake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = spark.sql(\"SELECT * FROM housing_market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLlib pipes\n",
    "MlLib allow us to create pipes such as those in sklearn.  \n",
    "Models usually receive arrays (or arrays of arrays) as input. \n",
    "The first thing we need to do is to convert the subset of features we are interessed in to a vector.  \n",
    "We can handle numerical and categorical features differently:  \n",
    "**numerical** can be imputed   \n",
    "**categorical** can be encoded \n",
    "\n",
    "### Task 1: Start by checking which variables do you need\n",
    "- we will use the sale price as our target\n",
    "- we will impute with mean all numerical features,\n",
    "- we will do one hot encoding on the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for non categorical variables, check distributions\n",
    "nonstring_features = [hm.columns[i] for i in range(len(hm.dtypes)) if hm.dtypes[i][1]!='string']\n",
    "string_features = [hm.columns[i] for i in range(len(hm.dtypes)) if hm.dtypes[i][1]=='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstring_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the __SalePrice__, as that is our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['SalePrice']\n",
    "## This might come in handy later\n",
    "all_nonstring_columns = nonstring_features.copy()\n",
    "nonstring_features.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstring_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nonstring_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Indexing categories\n",
    "https://spark.apache.org/docs/2.2.0/ml-features#stringindexer\n",
    "Gets a column with a categorical variable, and creates a new one -> with a name of our choice -> that converts the variables into ints  \n",
    "From the doc:\n",
    "\n",
    "\n",
    "\"Additionally, there are three strategies regarding how StringIndexer will handle unseen labels when you have fit a StringIndexer on one dataset and then use it to transform another:\n",
    "- throw an exception (which is the default)\n",
    "- skip the row containing the unseen label entirely\n",
    "- put unseen labels in a special additional bucket, at index numLabels\"\n",
    "\n",
    "In our case, we are will create a new column for each of the categorical variable and create a new one with the suffix _ix  \n",
    "Neighborhood - Neighborhood_ix    \n",
    "KitchenQual - KitchenQual_ix  \n",
    "Condition1 - Condition1_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, Imputer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index string features -> convert strings to numbers. The indexer detects number of different strings in the column and assigns a number to each of the strings.\n",
    "\n",
    "At this point, if you had 'null' results, you might want to consider additional cleaning, or just drop those elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=c, outputCol=c + '_ix', handleInvalid='keep') for c in string_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_indexers = Pipeline(stages=indexers)\n",
    "pipe = pipe_indexers.fit(df)\n",
    "\n",
    "df_indexer = pipe.transform(df)\n",
    "\n",
    "df_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexer.select(\"Condition1\", \"Condition1_ix\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Encoding categories\n",
    "\n",
    "We are going to create a new column for each of the categorical variables and create a new one with the suffix _cVec  \n",
    "\n",
    "Neighborhood_ix - Neighborhood_cVec   \n",
    "KitchenQual_ix - KitchenQual_cVec  \n",
    "Condition1_ix - Condition1_cVec\n",
    "\n",
    "This is what is called __one-hot encoding__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [OneHotEncoder(inputCol=col + \"_ix\", outputCol=col + \"_cVec\") for col in string_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_indexers = Pipeline(stages=encoders)\n",
    "pipe = pipe_indexers.fit(df_indexer)\n",
    "df_encoders = pipe.transform(df_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far is transforming \"Condition1\" into \"Consition1_cVec\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoders.select(\"Condition1\", \"Condition1_ix\", \"Condition1_cVec\").distinct().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Impute the numerical variables using a strategy.\n",
    "\n",
    "We need to tackle missing numerical values. For that we need a \"strategy\". It could be the median of all values, the mean, a constant value, like zero. We will use the mean and replace inplace. However,\n",
    "\n",
    "- Imputer only works with doubles or floats, so we need to recast all variables.\n",
    "- There are two types of strategy, mean and median  \n",
    "https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.feature.Imputer\n",
    "\n",
    "Let's recast all numeric columns as double."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoders = df_encoders.select([F.col(col).astype(\"double\") if col in all_nonstring_columns else col for col in df_encoders.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute the startegy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(strategy='mean', inputCols=nonstring_features, outputCols=nonstring_features)\n",
    "pipe_imputer = Pipeline(stages=[imputer]) ## stages need to be a sequence!! \n",
    "pipe = pipe_imputer.fit(df_encoders)\n",
    "\n",
    "df_imputer = pipe.transform(df_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Select the variables we want to set up our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the name of the column with a vector with all the assembled features\n",
    "assembled_col = 'assembled_features'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String feature cols after indexing\n",
    "enc_string_features = [c + '_cVec' for c in string_features]\n",
    "# Feature cols to assemble\n",
    "assemble_features = enc_string_features + nonstring_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble feature values to vector form\n",
    "assembler = VectorAssembler(inputCols=assemble_features, outputCol=assembled_col)\n",
    "pipe_assembler = Pipeline(stages=[assembler]) ## stages need to be a sequence!! \n",
    "pipe = pipe_assembler.fit(df_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled = pipe.transform(df_imputer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we're converting to Pandas so we can have a MarkDown preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, this is an example on how to deal with preparing a table to enable it to run be trained at scale. We did the table handling step-by-step. We can just do it in one go:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All tasks: Single pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_together = [imputer] + indexers + encoders + [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([F.col(col).astype(\"double\") if col in all_nonstring_columns else col for col in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_together = Pipeline(stages=stages_together) ## stages need to be a sequence!! \n",
    "pipe = pipe_together.fit(df)\n",
    "df_all_at_once = pipe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_at_once.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're ready to use the table\n",
    "\n",
    "All the steps we did so far were to make sure our numerical representation can be used to train a model. We just perfomed \"tidyness\" operations so far.\n",
    "\n",
    "Let's first use a Gradient Boosted Tree classifier, to see if we can predict the price of a house is above a certain threshold, giving the properties of the house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"target\", (F.col(\"SalePrice\")>100000.0).astype(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTClassifier(\n",
    "            featuresCol=assembled_col,\n",
    "            labelCol='target',\n",
    "            predictionCol='preds',\n",
    "            lossType='logistic',\n",
    "            maxDepth=4,\n",
    "            maxIter=20,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_for_train = [imputer] + indexers + encoders + [assembler] + [gbt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_together = Pipeline(stages=stages_for_train) ## stages need to be a sequence!! \n",
    "pipe = pipe_together.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = pipe.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.select(\"target\", \"probability\", \"preds\").toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf = spark.sql(\"SELECT * FROM housing_market\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(featuresCol=assembled_col,\n",
    "                      labelCol='SalePrice',\n",
    "                      maxIter=10,\n",
    "                      regParam=0.3,\n",
    "                      elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages_for_train = [imputer] + indexers + encoders + [assembler] + [lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_together = Pipeline(stages=stages_for_train) ## stages need to be a sequence!! \n",
    "#lr_df = pipe_together.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = pipe_together.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df.stages[-1].summary.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
